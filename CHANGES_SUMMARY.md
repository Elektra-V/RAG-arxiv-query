# Project Changes Summary & How It Works

## ğŸ“‹ Overview

This project has been **simplified and focused** on using the company LLM provider (Fraunhofer GenAI) with proper authentication following the official API documentation.

## ğŸ”„ Key Changes Made

### 1. **Simplified Configuration** (`env.example`)
- âœ… Focused ONLY on company API configuration
- âœ… Removed confusing multiple provider options
- âœ… Clear sections with comments
- âœ… Instructions to check available models first

**Before**: Mixed configuration for Ollama, OpenAI, Anthropic, etc.  
**After**: Clean, focused company API setup

### 2. **Centralized OpenAI Client** (`rag_api/clients/openai.py`)
- âœ… **NEW FILE**: Single source of truth for authentication
- âœ… Follows EXACT pattern from company API documentation:
  ```python
  token_string = f"{username}:{password}"
  token_bytes = b64encode(token_string.encode())
  client = OpenAI(
      api_key="xxxx",
      default_headers={"Authorization": f"Basic {token_bytes.decode()}"},
      base_url="https://genai.iais.fraunhofer.de/api/v2"
  )
  ```
- âœ… All services use the same client factory
- âœ… Easy to debug - all auth logic in one place

### 3. **Model Checker Script** (`check_company_models.py`)
- âœ… **NEW FILE**: Checks what models are available on gateway
- âœ… Tests your credentials before running
- âœ… Lists all available models
- âœ… Prevents errors from using wrong model names

### 4. **Simplified Service Code**
All services now use the centralized client:
- `rag_api/services/langchain/agent.py` - Uses `get_openai_client()`
- `rag_api/services/llamaindex/index.py` - Uses `get_openai_client()`
- `rag_api/clients/embeddings.py` - Uses `get_openai_client()`

**Before**: Each service had its own auth logic (duplicated)  
**After**: One client factory, reused everywhere

### 5. **Documentation**
- âœ… `README_COMPANY_API.md` - Focused company API guide
- âœ… Simplified `README.md` with clear quick start
- âœ… `env.example` has clear comments and instructions

## ğŸ—ï¸ How The Project Works Now

### Architecture Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    .env file                             â”‚
â”‚  (Credentials: username, password, base_url, model)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              rag_api/settings.py                        â”‚
â”‚         (Reads from .env, provides Settings)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         rag_api/clients/openai.py                       â”‚
â”‚                                                          â”‚
â”‚  get_openai_client()                                    â”‚
â”‚    â”œâ”€ Reads credentials from settings                   â”‚
â”‚    â”œâ”€ Encodes username:password as Base64               â”‚
â”‚    â”œâ”€ Creates OpenAI client with Basic auth header      â”‚
â”‚    â””â”€ Returns configured client                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                       â”‚              â”‚
        â–¼                       â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LangChain    â”‚   â”‚  LlamaIndex  â”‚  â”‚  Embeddings  â”‚
â”‚  Service      â”‚   â”‚  Service     â”‚  â”‚  Client      â”‚
â”‚               â”‚   â”‚              â”‚  â”‚              â”‚
â”‚  Uses client  â”‚   â”‚  Uses client â”‚  â”‚  Uses client â”‚
â”‚  from factory â”‚   â”‚  from factoryâ”‚  â”‚  from factoryâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Authentication Flow (Following Company API Pattern)

```
1. User sets credentials in .env:
   OPENAI_AUTH_USERNAME="my-username"
   OPENAI_AUTH_PASSWORD="my-password"
   OPENAI_BASE_URL="https://genai.iais.fraunhofer.de/api/v2"

2. get_openai_client() is called:
   
   a) Reads credentials from settings
   
   b) Creates token string:
      token_string = f"{username}:{password}"
   
   c) Encodes as Base64:
      token_bytes = b64encode(token_string.encode())
   
   d) Creates OpenAI client:
      client = OpenAI(
          api_key="xxxx",
          default_headers={
              "Authorization": f"Basic {token_bytes.decode()}"
          },
          base_url="https://genai.iais.fraunhofer.de/api/v2"
      )

3. Client is used by all services:
   - LangChain agent â†’ uses client for LLM
   - LlamaIndex â†’ uses client for LLM
   - Embeddings â†’ uses client for embeddings
```

## ğŸ“ Step-by-Step Usage

### Initial Setup

1. **Copy environment template**:
   ```bash
   cp env.example .env
   ```

2. **Edit `.env` with your credentials**:
   ```env
   OPENAI_AUTH_USERNAME="your-username"
   OPENAI_AUTH_PASSWORD="your-password"
   OPENAI_BASE_URL="https://genai.iais.fraunhofer.de/api/v2"
   ```

3. **Check available models** (IMPORTANT):
   ```bash
   uv run python check_company_models.py
   ```
   
   Output will show:
   ```
   âœ… Connection successful!
   ğŸ“‹ Available models:
   âœ“ Llama-3-SauerkrautLM
   âœ“ Llama-3-8B-Instruct
   ...
   ```

4. **Update model name in `.env`**:
   ```env
   OPENAI_MODEL="Llama-3-SauerkrautLM"  # Use one from the list
   ```

5. **Install dependencies**:
   ```bash
   uv sync
   ```

6. **Start service**:
   ```bash
   uv run rag_api/services/langchain/app.py
   ```

### Running the Services

**LangChain Service** (port 8009):
```bash
uv run rag_api/services/langchain/app.py
```
- Open: http://localhost:8009/
- Query endpoint: `POST /query`
- Status: `GET /status`

**LlamaIndex Service** (port 8080):
```bash
uv run rag_api/services/llamaindex/app.py
```
- Open: http://localhost:8080/
- Query endpoint: `POST /query`
- Status: `GET /status`

## ğŸ” Key Files & Their Roles

### Configuration Files
- **`env.example`** - Template with company API configuration
- **`.env`** - Your actual credentials (not in git)

### Core Authentication
- **`rag_api/clients/openai.py`** - Centralized client factory
  - `create_openai_client()` - Creates client with auth
  - `get_openai_client()` - Main entry point (uses settings)

### Settings
- **`rag_api/settings.py`** - Reads from `.env`, provides Settings class

### Services (All use the same client)
- **`rag_api/services/langchain/agent.py`** - LangChain agent service
- **`rag_api/services/llamaindex/index.py`** - LlamaIndex service
- **`rag_api/clients/embeddings.py`** - Embeddings client

### Utilities
- **`check_company_models.py`** - Check available models on gateway
- **`check_setup.py`** - Validate project setup

## âœ… Benefits of This Architecture

1. **Single Source of Truth**: All auth logic in `rag_api/clients/openai.py`
2. **Easy Debugging**: Check one file to see how auth works
3. **Follows Official Pattern**: Matches company API documentation exactly
4. **No Duplication**: One client factory, reused everywhere
5. **Type Safety**: Proper typing throughout
6. **Clear Separation**: Settings â†’ Client Factory â†’ Services

## ğŸ”§ Troubleshooting

**"Can't connect to API"**
- Run `check_company_models.py` to test credentials
- Verify `.env` has correct username/password
- Check `OPENAI_BASE_URL` is correct

**"Model not found"**
- Run `check_company_models.py` to see available models
- Update `OPENAI_MODEL` in `.env` with correct model name

**"Authentication fails"**
- Check `rag_api/clients/openai.py` for encoding logic
- Verify credentials in `.env`
- Check logs for specific error

## ğŸ“Š What Changed vs. Before

| Aspect | Before | After |
|--------|--------|-------|
| **Configuration** | Multiple providers, confusing | Focused on company API only |
| **Auth Logic** | Duplicated in each service | Centralized in one file |
| **Model Discovery** | Manual guesswork | Script to check available models |
| **Documentation** | Scattered across files | Clear, focused guides |
| **Debugging** | Hard to find auth issues | All auth in `openai.py` |

## ğŸ¯ Next Steps

1. âœ… Set up `.env` with your credentials
2. âœ… Run `check_company_models.py` to verify connection
3. âœ… Update `OPENAI_MODEL` with available model
4. âœ… Start service and test
5. âœ… Use debug UI at http://localhost:8009/

---

**The project is now simplified, focused, and ready for company API usage!** ğŸš€

